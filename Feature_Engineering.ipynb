{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is a parameter?\n",
        "- A parameter is a numerical value that summarizes a characteristic of a population in statistics.\n",
        "\n",
        "More Specifically:\n",
        ". It is a fixed (but often unknown) quantity, such as:\n",
        "\n",
        ". Mean (Œº) ‚Äì average of the population\n",
        "\n",
        ". Proportion (p) ‚Äì proportion of the population with a specific characteristic\n",
        ". Standard deviation (œÉ) ‚Äì measure of spread in the population\n",
        "\n",
        "\n",
        "\n",
        "2. What is correlation?\n",
        " What does negative correlation mean?\n",
        "- Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        " What Does Negative Correlation Mean?\n",
        "A negative correlation means that as one variable increases, the other tends to decrease.\n",
        "\n",
        "\n",
        "\n",
        "3.  Define Machine Learning. What are the main components in Machine Learning?\n",
        "Machine Learning (ML) is a subset of artificial intelligence (AI) that enables computers to learn from data and improve their performance on a task without being explicitly programmed.\n",
        "\n",
        "Instead of writing specific instructions, you provide data, and the machine learns patterns or makes predictions.\n",
        "\n",
        " Main Components of Machine Learning\n",
        "Here are the key components that make up a typical machine learning system:\n",
        "\n",
        "1. Data\n",
        "Raw information used for training and testing.\n",
        "\n",
        "Can be structured (e.g., tables) or unstructured (e.g., text, images).\n",
        "\n",
        "2. Features\n",
        "Measurable input variables extracted from the data.\n",
        "\n",
        "Example: For a house price model, features might be number of rooms, location, size.\n",
        "\n",
        "3. Model\n",
        "A mathematical representation that maps inputs (features) to outputs (predictions).\n",
        "\n",
        "Example: Linear regression, decision tree, neural network.\n",
        "\n",
        "4. Algorithm\n",
        "A method or process used to train the model (i.e., how the model learns from data).\n",
        "\n",
        "Examples: Gradient Descent, k-Nearest Neighbors, Random Forest.\n",
        "\n",
        "5. Training\n",
        "The process of feeding data to the model so it can learn.\n",
        "\n",
        "Involves minimizing a loss function (error) by adjusting internal parameters.\n",
        "\n",
        "6. Evaluation\n",
        "Testing the model on unseen data (test set) to assess performance.\n",
        "\n",
        "Common metrics: accuracy, precision, recall, mean squared error.\n",
        "\n",
        "7. Prediction\n",
        "Once trained, the model makes predictions on new, unseen data.\n",
        "\n",
        "8. Feedback / Tuning\n",
        "Use performance metrics to fine-tune the model (e.g., through hyperparameter tuning or retraining).\n",
        "\n",
        "Would you like a visual diagram or a basic example in Python to see how this fits together?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4.  How does loss value help in determining whether the model is good or not?\n",
        "- 1.  Training:\n",
        "\n",
        "During training, the model adjusts its parameters (weights) to minimize the loss.\n",
        "\n",
        "A decreasing loss over time (epochs) means the model is learning.\n",
        "\n",
        "2. Model Selection:\n",
        "\n",
        "You can compare loss values between different models (or versions of a model) to choose the one that performs best.\n",
        "\n",
        "3. Overfitting Detection:\n",
        "\n",
        "If training loss is low but validation loss is high, your model is likely overfitting ‚Äî it performs well on training data but poorly on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "-  Continuous Variables:\n",
        "Definition: Variables that can take any numerical value within a range.\n",
        "\n",
        "These values are measurable and can have fractions or decimals.\n",
        "\n",
        "They usually represent quantities.\n",
        "\n",
        "Categorical Variables:\n",
        "Definition: Variables that represent groups or categories.\n",
        "\n",
        "These values are not numeric in nature (even if coded with numbers).\n",
        "\n",
        "They usually represent types, labels, or qualities.\n",
        "\n",
        "6.  How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "- Handling categorical variables in machine learning is critical because most algorithms require numerical input. To use categorical data effectively, we must convert it into a numerical format.\n",
        "\n",
        " Common Techniques to Handle Categorical Variables:\n",
        "1. Label Encoding\n",
        "Converts each category into a unique integer.\n",
        "\n",
        "Good for ordinal variables (where order matters).\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Converts each category into a separate binary column (0 or 1).\n",
        "\n",
        "Best for nominal variables.\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Maps categories to integers with order preserved.\n",
        "4. Binary Encoding\n",
        "Combination of hashing and one-hot encoding.\n",
        "\n",
        "Reduces dimensionality for high-cardinality features.\n",
        "5. Target Encoding (Mean Encoding)\n",
        "Replaces categories with the mean of the target variable for each category.\n",
        "\n",
        "\n",
        "7.  What do you mean by training and testing a dataset?\n",
        "-  In machine learning, the dataset is usually split into two (or more) parts:\n",
        "\n",
        "1. Training Dataset\n",
        "This is the portion of the data used to train the model.\n",
        "\n",
        "The model learns patterns, relationships, and parameters from this data.\n",
        "\n",
        "It adjusts its internal weights or rules to minimize error.\n",
        "2.  Testing Dataset\n",
        "This is separate data that the model has never seen before.\n",
        "\n",
        "Used to evaluate how well the model performs on unseen data.\n",
        "\n",
        "Helps measure how well the model generalizes to new inputs.\n",
        "\n",
        "\n",
        "8.  What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module in scikit-learn that provides tools for preparing your data before training machine learning models.\n",
        "\n",
        "Preprocessing helps improve the performance and accuracy of models by transforming the data into a more suitable format.\n",
        "\n",
        "\n",
        "9.  What is a Test set?\n",
        "-  The test set is a portion of your dataset that is used to evaluate the performance of a trained machine learning model on unseen data.\n",
        "\n",
        "\n",
        "10.  How do we split data for model fitting (training and testing) in Python?\n",
        " How do you approach a Machine Learning problem?\n",
        " - Use train_test_split from scikit-learn:\n",
        " . test_size=0.2 means 20% data for testing, 80% for training.\n",
        "\n",
        ". random_state ensures reproducibility.\n",
        "\n",
        ". Optionally use stratify=y to keep class proportions (important for classification).\n",
        "\n",
        "How do you approach a Machine Learning problem?\n",
        "1. Understand the problem\n",
        "Define the goal: classification, regression, clustering?\n",
        "\n",
        "Collect and prepare data\n",
        "Clean data, handle missing values, and preprocess.\n",
        "\n",
        "Explore the data (EDA)\n",
        "Visualize and analyze feature distributions and relationships.\n",
        "\n",
        "Feature engineering\n",
        "Encode categorical variables, scale features, create new features.\n",
        "\n",
        "Split the dataset\n",
        "Divide into training and testing sets.\n",
        "\n",
        "Select and train a model\n",
        "Choose algorithm(s) and fit on training data.\n",
        "\n",
        "Evaluate the model\n",
        "Test on unseen data and calculate performance metrics.\n",
        "\n",
        "Tune and improve\n",
        "Optimize hyperparameters, try other models or feature sets.\n",
        "\n",
        "Deploy and monitor\n",
        "Use the model in production and track its performance over time.\n",
        "\n",
        "\n",
        "11.  Why do we have to perform EDA before fitting a model to the data?\n",
        "-1.  Understand the Data\n",
        "EDA helps you grasp the structure, patterns, and relationships in your data. Without this, you risk making wrong assumptions.\n",
        "2. Detect Data Quality Issues\n",
        "You can identify missing values, outliers, duplicates, or incorrect data types early on, which might skew your model‚Äôs results.\n",
        "\n",
        "3. Inform Feature Engineering\n",
        "Insights from EDA guide you on how to transform, encode, or create new features to improve model performance.\n",
        "\n",
        "4. Choose the Right Model\n",
        "Knowing your data distribution and relationships helps in selecting appropriate algorithms and model parameters.\n",
        "\n",
        "5. Avoid Garbage In, Garbage Out\n",
        "Feeding raw or poorly understood data into models can lead to bad predictions. EDA ensures you clean and prepare data properly.\n",
        "\n",
        "6. Detect Class Imbalance or Bias\n",
        "If your target classes are imbalanced, EDA reveals this so you can apply techniques like resampling.\n",
        "\n",
        "\n",
        "12. What is correlation?\n",
        "- Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "- Negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa.\n",
        "\n",
        "\n",
        "14.  How can you find correlation between variables in Python?\n",
        "- You can use Pandas .corr() method for DataFrames or NumPy‚Äôs corrcoef function for arrays.\n",
        "\n",
        "\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example\n",
        "-  Causation means that one event or variable directly causes a change in another. In other words, a change in variable A produces a change in variable B.\n",
        "What is the difference between correlation and causation?\n",
        "correlation:\n",
        ". Shows a relationship or association between two variables\n",
        ". Does not imply one variable causes the othe\n",
        ". Can be positive, negative, or zero\n",
        ". Can be due to coincidence or other factors (confounders)\n",
        "\n",
        "Causation:\n",
        ". Shows a direct cause-and-effect relationship\n",
        ". Implies one variable directly affects the other\n",
        ". Involves a mechanism or reason for the effect\n",
        ". Requires evidence of causal link\n",
        "Example:\n",
        ". Correlation example: Ice cream sales and drowning incidents both increase in summer. They are correlated but ice cream sales do not cause drownings; the real factor is the warmer weather.\n",
        "\n",
        ". Causation example: Smoking causes an increase in lung cancer risk. Here, smoking directly causes changes in health outcomes.\n",
        "\n",
        "\n",
        "\n",
        "16.  What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "- An optimizer is an algorithm or method used to adjust the parameters (like weights) of a machine learning model during training to minimize the loss function (error). It helps the model learn by finding the best set of parameters that reduce prediction errors.\n",
        "There are several popular optimizers, especially in neural networks:\n",
        "1. Gradient Descent (GD)\n",
        "What: The basic optimization method that updates parameters in the direction of the negative gradient of the loss function.\n",
        "\n",
        "How: Uses the entire dataset to compute gradients at each step.\n",
        "Example:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j_jHLW13IrXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Pros: Simple and stable.\n",
        "\n",
        ". Cons: Can be slow for large datasets because it processes all data before each update.\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "What: Updates parameters using the gradient calculated from a single randomly chosen data point or a small batch, rather than the entire dataset.\n",
        "\n",
        "Pros: Faster updates, can escape local minima.\n",
        "\n",
        "Cons: More noisy updates, which can cause instability.\n",
        "\n",
        "Example:\n"
      ],
      "metadata": {
        "id": "HvzoAGRAREqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for each sample in dataset:\n",
        "    theta = theta - learning_rate * gradient(sample)\n"
      ],
      "metadata": {
        "id": "S1JEyEyhTJ-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Mini-batch Gradient Descent\n",
        "What: A middle ground that uses a small batch of samples to compute the gradient at each step.\n",
        "\n",
        "Pros: Faster than GD, more stable than SGD.\n",
        "\n",
        "Example: Typical batch size is 32 or 64 samples.\n",
        "\n",
        "4. Momentum\n",
        "What: Accelerates SGD by adding a fraction of the previous update to the current update, helping to navigate valleys and reduce oscillations.\n",
        "\n",
        "Update rule:\n",
        "\n",
        "ùë£\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "ùë£\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        ")\n",
        "‚àá\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤v\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +(1‚àíŒ≤)‚àáJ(Œ∏)\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùõº\n",
        "ùë£\n",
        "ùë°\n",
        "Œ∏=Œ∏‚àíŒ±v\n",
        "t\n",
        "‚Äã\n",
        "\n",
        "Example:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mr6BShmLR1YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "velocity = beta * velocity + (1 - beta) * gradient\n",
        "theta = theta - learning_rate * velocity\n"
      ],
      "metadata": {
        "id": "X15Ftu1qSaw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  AdaGrad\n",
        "What: Adapts learning rates for each parameter individually, performing larger updates for infrequent parameters and smaller updates for frequent ones.\n",
        "\n",
        "Useful for: Sparse data.\n",
        "\n",
        "Limitation: Learning rate can shrink too much over time.\n",
        "\n",
        "6. RMSProp\n",
        "What: Modifies AdaGrad to use a moving average of squared gradients to normalize updates, avoiding AdaGrad‚Äôs diminishing learning rates.\n",
        "Popular in: Deep learning.\n",
        "\n",
        "7. Adam (Adaptive Moment Estimation)\n",
        "What: Combines momentum and RMSProp. Keeps track of exponentially decaying averages of past gradients and squared gradients.\n",
        "\n",
        "Widely used because: It generally works well and requires less tuning.\n",
        "\n",
        "Update rule (simplified):"
      ],
      "metadata": {
        "id": "5Z2RDNYQSjQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = beta1 * m + (1 - beta1) * gradient\n",
        "v = beta2 * v + (1 - beta2) * (gradient ** 2)\n",
        "theta = theta - learning_rate * m / (sqrt(v) + epsilon)\n"
      ],
      "metadata": {
        "id": "88-xgiuQSxiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  What is sklearn.linear_model ?\n",
        "- sklearn.linear_model is a module in the scikit-learn Python library that provides classes and functions to implement various linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable.\n",
        "\n",
        "\n",
        "\n",
        "18.  What does model.fit() do? What arguments must be given?\n",
        "- The model.fit() method trains or fits a machine learning model to the given data. It adjusts the model parameters so that the model learns the relationship between input features and the target variable.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "X: The input features (usually a 2D array or DataFrame), shape (n_samples, n_features).\n",
        "\n",
        "y: The target values or labels (usually a 1D array or Series), shape (n_samples,).\n",
        "\n",
        "\n",
        "\n",
        "19.  What does model.predict() do? What arguments must be given?\n",
        "-  The model.predict() method uses a trained model to make predictions on new input data. It applies the learned parameters from fit() to estimate the target values.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "X: The input features (2D array or DataFrame) for which you want predictions, shape (n_samples, n_features).\n",
        "\n",
        "It returns an array of predicted values or labels corresponding to each input sample.\n",
        "\n",
        "\n",
        "20.  What are continuous and categorical variables?\n",
        "-  Continuous variables:\n",
        "These are variables that can take any value within a range. They are numerical and often measured. Examples include height, temperature, weight, or age.\n",
        "\n",
        "Categorical variables:\n",
        "These are variables that represent distinct categories or groups. They are qualitative and usually have a limited set of values or labels. Examples include gender (male/female), color (red/blue/green), or type of car (SUV/sedan/truck).\n",
        "\n",
        "\n",
        "21.  What is feature scaling? How does it help in Machine Learning?\n",
        "- Feature scaling is a preprocessing technique that normalizes or standardizes the range of independent variables (features) in your dataset.\n",
        "\n",
        "It transforms features so they have a similar scale, typically by rescaling to a fixed range (like 0 to 1) or adjusting to have a mean of 0 and standard deviation of 1.\n",
        "\n",
        "Why is feature scaling important?\n",
        "Many ML algorithms (like gradient descent, SVM, KNN, and neural networks) perform better or converge faster when features are on similar scales.\n",
        "\n",
        "Prevents features with large values from dominating those with smaller values.\n",
        "\n",
        "Helps algorithms that calculate distances (e.g., KNN, clustering) to treat all features equally.\n",
        "\n",
        "Common methods of feature scaling:\n",
        "Min-Max Scaling: Scales features to [0, 1] range.\n",
        "\n",
        "Standardization (Z-score normalization): Scales features to have mean = 0 and std = 1.\n",
        "\n",
        "\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "-  We typically use scikit-learn‚Äôs preprocessing module to perform feature scaling easily. The two most common methods are:\n",
        "\n",
        "1. Min-Max Scaling\n",
        "Scales features to a fixed range, usually [0, 1].\n",
        "\n",
        "2. Standardization (Z-score Normalization)\n",
        "Centers features by removing the mean and scales to unit variance.\n",
        "\n",
        "How to use:\n",
        "fit_transform() computes the parameters (min, max, mean, std) on training data and transforms it.\n",
        "\n",
        "For test data, use transform() to apply the same scaling learned from training.\n",
        "\n",
        "\n",
        "23.  What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module in the scikit-learn Python library that provides tools and functions to transform and prepare data before feeding it into machine learning models.\n",
        "\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "-  In Python, we typically use scikit-learn‚Äôs train_test_split function to divide data into training and testing sets. This allows us to train the model on one portion of the data and evaluate its performance on unseen data.\n",
        "\n",
        "Explanation of parameters:\n",
        "X, y: The full dataset features and labels.\n",
        "\n",
        "test_size=0.2: 20% of data reserved for testing, 80% for training.\n",
        "\n",
        "random_state: Seed for reproducibility of the split.\n",
        "Why split data?\n",
        "To train the model on one set (training set).\n",
        "\n",
        "To test the model‚Äôs performance on unseen data (testing set), ensuring it generalizes well.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "25. Explain data encoding?\n",
        "- Data encoding is the process of transforming categorical variables into a numerical format so that machine learning algorithms can process them. Since most algorithms work with numbers, encoding is essential to convert text labels or categories into numbers.\n",
        "\n"
      ],
      "metadata": {
        "id": "nc_wAv3JTf3d"
      }
    }
  ]
}